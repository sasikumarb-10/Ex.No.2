
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE : 29\08\2025                                                                            
### REGISTER NUMBER : 212223060252
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

### AI Tools required:
- OpenAI ChatGPT  
- Anthropic Claude  
- Google Bard (Gemini)  
- Cohere Command  
- Meta AI  

### Explanation:
1. **Define the Use Case**  
   A **text summarization task** was chosen as the common use case. This allows testing the ability of different AI platforms to condense long passages while retaining meaning and relevance.  

2. **Create a Set of Prompts**  
   - *Prompt 1:* Summarize the following paragraph into 3 lines.  
   - *Prompt 2:* Write a concise 50-word summary highlighting only the key points.  
   - *Prompt 3:* Act as a professional editor and provide a short summary of the text.  

3. **Run the Experiment on Each AI Platform**  
   - Each summarization prompt was tested on ChatGPT, Claude, Bard (Gemini), Cohere Command, and Meta AI.  
   - Responses were collected under the same conditions.  
   - Observed: response speed, summary precision, readability, and adaptability to prompt style.  

4. **Evaluate Response Quality**  
   - **Accuracy**: How well the summary preserved the core information.  
   - **Clarity**: Simplicity and readability of the summarized text.  
   - **Conciseness**: Ability to shorten without losing meaning.  
   - **Adaptability**: How effectively the model followed different prompting techniques.  

### Output:
| **Criteria**     | **ChatGPT** | **Claude** | **Bard (Gemini)** | **Cohere Command** | **Meta AI** |
|------------------|-------------|------------|-------------------|---------------------|-------------|
| **Accuracy**     | 9/10 – Preserves all key points | 9/10 – Very precise | 8/10 – Sometimes verbose | 7/10 – Misses minor details | 7/10 – Simplified version |
| **Clarity**      | 9/10 – Clear and structured | 10/10 – Smooth and human-like | 8/10 – Clear but less formal | 7/10 – Short but vague | 7/10 – Easy but basic |
| **Conciseness**  | 8/10 – Balanced detail | 9/10 – Very crisp | 8/10 – Sometimes too long | 7/10 – Compact but shallow | 6/10 – Overly short |
| **Adaptability** | 9/10 – Adjusts well to styles | 9/10 – Very responsive | 8/10 – Moderate adaptability | 7/10 – Limited flexibility | 6/10 – Rigid |
| **Response Time**| Fast | Medium-Fast | Fast | Fast | Medium |
| **Ease of Use**  | Excellent (UI + API) | Excellent (Conversational) | Good (Google ecosystem) | Good (Developer tools) | Good (Basic use) |  

### Conclusion: 
- **Claude** delivered the clearest and most concise summaries, making it best for professional use.  
- **ChatGPT** performed strongly in both accuracy and adaptability, making it reliable across scenarios.  
- **Bard (Gemini)** produced creative summaries but sometimes added unnecessary details.  
- **Cohere Command** gave short outputs but lacked depth.  
- **Meta AI** simplified too much, leading to occasional loss of context.

  For precise professional summarization, Claude and ChatGPT are most effective.  
  For creative summarization, Bard is a better choice.  
  For quick developer-focused summaries, Cohere works well.  
  For casual simple summaries, Meta AI is sufficient.  

## References:  
1. OpenAI. (2024). *ChatGPT Documentation*. [https://platform.openai.com/](https://platform.openai.com/)  
2. Anthropic. (2024). *Claude AI*. [https://www.anthropic.com/](https://www.anthropic.com/)  
3. Google DeepMind. (2024). *Gemini (Bard)*. [https://bard.google.com/](https://bard.google.com/)  
4. Cohere. (2024). *Cohere Command*. [https://cohere.com/](https://cohere.com/)  
5. Meta AI. (2024). *Meta AI Research*. [https://ai.meta.com/](https://ai.meta.com/)  


# Result : The Prompt for the above problem statement executed successfully.
